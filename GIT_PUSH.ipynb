{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b5abc0",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages & set display options\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ccb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append true labels at specified offsets from future observations of session\n",
    "# dependent on whether the future session time exists in the session or not\n",
    "\n",
    "def prep_df(uids, df, offset, result):\n",
    "    list_uids = list(uids['SESSION_UID'][uids['DURATION (MINS)'] > int(offset)])\n",
    "    for i in list_uids:\n",
    "        filt_df = df[(df['SESSION_UID'] == i)]\n",
    "        filt_df = filt_df[(filt_df['TIME_OFFSET'] == 0) | (filt_df['TIME_OFFSET'] == int(offset))]\n",
    "        filt_df['SESSION_TIME'] = filt_df['SESSION_TIME'].apply(np.floor)\n",
    "        filt_df = filt_df.drop_duplicates()\n",
    "        filt_df = filt_df.sort_values(by='SESSION_TIME')\n",
    "        filt_df['FWD'] = filt_df['SESSION_TIME'] + (60 * int(offset))\n",
    "        \n",
    "        fwd = filt_df[filt_df['TIME_OFFSET'] == 0]\n",
    "        fwd = fwd[['FWD', 'RAIN_PERCENTAGE','WEATHER']]\n",
    "        fwd = fwd.rename(columns={'FWD':'SESSION_TIME',\n",
    "                          'WEATHER': 'PRED_WEATHER',\n",
    "                          'RAIN_PERCENTAGE': 'PRED_RAIN_PERCENTAGE'})\n",
    "        \n",
    "        curr = filt_df[filt_df['TIME_OFFSET'] == int(offset)]\n",
    "        \n",
    "        tot = curr.merge(fwd, on=['SESSION_TIME'], how='inner')\n",
    "        tot = tot.drop(columns=['FWD', 'SESSION_TYPE', 'N_FORECASTS']) \n",
    "        result = result.append(tot)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Function to utilise session forecast values for weather & precip as ground truth labels \n",
    "\n",
    "def offset_prep(offset_df, current_df):\n",
    "    offset_num = int(offset_df.loc[offset_df.index[0],'TIME_OFFSET'])\n",
    "    current_df = current_df.drop(columns=['SESSION_TYPE', 'TIME_OFFSET', 'N_FORECASTS', \n",
    "                                          'TRACK_TEMPERATURE', 'TRACK_TEMP_CHANGE', \n",
    "                                          'AIR_TEMPERATURE', 'AIR_TEMP_CHANGE', \n",
    "                                          'FORECAST_TRACK_TEMP', 'FORECAST_AIR_TEMP', \n",
    "                                          'FORECAST_WEATHER'])\n",
    "    offset_df = offset_df.drop(columns=['SESSION_TYPE', 'N_FORECASTS'])\n",
    "    offset_df = offset_df.rename(columns={col: 'PRED_' + col \n",
    "                        for col in offset_df.columns if col in ['WEATHER', 'RAIN_PERCENTAGE']})\n",
    "    forecast_df = current_df.merge(offset_df, on=['SESSION_UID', 'SESSION_TIME'], how='inner')\n",
    "    #forecast_df = forecast_df.drop(columns=['TIME_OFFSET'])\n",
    "    forecast_df = forecast_df[['SESSION_UID', 'SESSION_TIME',  'TIME_OFFSET', 'TRACK_TEMPERATURE',\n",
    "                             'TRACK_TEMP_CHANGE', 'AIR_TEMPERATURE', 'AIR_TEMP_CHANGE',\n",
    "                             'FORECAST_TRACK_TEMP', 'FORECAST_AIR_TEMP', \"FORECAST_WEATHER\",\n",
    "                             'RAIN_PERCENTAGE', 'WEATHER', 'PRED_RAIN_PERCENTAGE', 'PRED_WEATHER']]\n",
    "    return forecast_df\n",
    "\n",
    "\n",
    "# Function to create split dataset into x & y, with separate datasets for both weather and\n",
    "# precipitation target variables\n",
    "\n",
    "def split_targets(df, smote):\n",
    "    sm = SMOTE(random_state = 2)\n",
    "    prec = df.drop(columns='PRED_WEATHER')\n",
    "    weather = df.drop(columns='PRED_RAIN_PERCENTAGE')\n",
    "    \n",
    "    prec_train_x = prec.drop(columns=['PRED_RAIN_PERCENTAGE'])\n",
    "    prec_train_y = prec['PRED_RAIN_PERCENTAGE']\n",
    "    \n",
    "    weath_train_x = weather.drop(columns=['PRED_WEATHER'])\n",
    "    weath_train_y = weather['PRED_WEATHER']\n",
    "    if smote == 1:\n",
    "        print('BEFORE:', Counter(weath_train_y))\n",
    "        weath_train_x, weath_train_y = sm.fit_resample(weath_train_x, weath_train_y.ravel())\n",
    "        print('AFTER:', Counter(weath_train_y))\n",
    "    else:\n",
    "        print('NO SMOTE:', Counter(weath_train_y))\n",
    "    return prec_train_x, prec_train_y, weath_train_x, weath_train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ef570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset as .csv\n",
    "df = pd.read_csv('weather.csv', index_col=False, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa35f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names for readibility\n",
    "\n",
    "df.columns = df.columns.str.replace(r'^M_', '', regex=True)\n",
    "# names = list(df.columns)\n",
    "# print(names, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9709a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns based on assumptions:\n",
    "# 1. Game-setting relevance only (AI difficulty, DRSASSIST, etc)\n",
    "# 2. Not having enough information to incorporate into the shipped product. Columns referencing pit stop\n",
    "#    windows, safety cars etc lack additional information to make use of (such as for strategy)\n",
    "# 3. Time/session related identifiers (such as season/session/weekend link identifier) - which we\n",
    "#    could not establish a relationship with the limited data we had available.\n",
    "\n",
    "df1 = df.drop(columns=['PACKET_FORMAT', 'GAME_MAJOR_VERSION', 'PACKET_VERSION', 'PACKET_ID', \n",
    "                      'SECONDARY_PLAYER_CAR_INDEX', 'SLI_PRO_NATIVE_SUPPORT', 'SAFETY_CAR_STATUS', \n",
    "                      'DRSASSIST', 'STEERING_ASSIST', 'AI_DIFFICULTY', 'NETWORK_GAME', 'PIT_RELEASE_ASSIST',\n",
    "                      'BRAKING_ASSIST', 'GAME_MINOR_VERSION', 'ERSASSIST', 'PIT_ASSIST', 'GEARBOX_ASSIST',\n",
    "                      'DYNAMIC_RACING_LINE', 'DYNAMIC_RACING_LINE_TYPE', 'PIT_SPEED_LIMIT', 'SPECTATOR_CAR_INDEX',\n",
    "                      'FRAME_IDENTIFIER', 'GAMEHOST', 'ZONE_START', 'ZONE_FLAG', 'PIT_STOP_REJOIN_POSITION',\n",
    "                      'NUM_MARSHAL_ZONES', 'SEASON_LINK_IDENTIFIER', 'WEEKEND_LINK_IDENTIFIER', \n",
    "                      'SESSION_LINK_IDENTIFIER','PIT_STOP_WINDOW_LATEST_LAP', 'Unnamed: 58', 'TIMESTAMP',\n",
    "                      'FORMULA', 'PLAYER_CAR_INDEX', 'TOTAL_LAPS', 'TRACK_LENGTH']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "632ce1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering & cleaning of data set rows based on some assumptions, such as the omission of paused packets,\n",
    "# and the removal of \"approximate\" weather forecast settings, which, with some domain research, removes\n",
    "# the dynamic nature of weather and makes it more 'predictable.'\n",
    "\n",
    "# Removal of rows which are potentially \n",
    "# forecasting future sessions or days weather. Only leaving immediate session forecasts.\n",
    "df1 = df1[(df1['NUM_WEATHER_FORECAST_SAMPLES'] != 0) & (df1['SESSION_TYPE'] != 0)] \n",
    "df1 = df1[df1['SESSION_TYPE'] == df1['WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE']] \n",
    "\n",
    "df1 = df1[df1['GAME_PAUSED'] == 0]\n",
    "\n",
    "#df1 = df1[df1['FORECAST_ACCURACY'] == 0] # lose 9% of vals on duplicate drop\n",
    "\n",
    "# Remove additional columns after row-wise filtering considerations\n",
    "df_clean = df1.drop(columns=['GAME_PAUSED', 'IS_SPECTATING', 'SESSION_TIME_LEFT', 'PIT_STOP_WINDOW_IDEAL_LAP',\n",
    "                             'SESSION_DURATION', 'FORECAST_ACCURACY', 'WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE',\n",
    "                             'TRACK_ID'])\n",
    "\n",
    "df_clean = df_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ad98e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 165562 \n",
      "current: 165522\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean for duplicated SESSION_TIME & TIME_OFFSET within each SESSION_UID. \n",
    "# Aiming to achieve a row for each second, and this removes some duplicate session time/UIDs\n",
    "# which have different values in the feature columns and instead calculates the mean.\n",
    "\n",
    "orig = len(df_clean)\n",
    "df_clean = df_clean.groupby(['SESSION_UID', 'SESSION_TIME', 'TIME_OFFSET']).mean().reset_index()\n",
    "print('original:', orig , '\\ncurrent:', len(df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f6b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder data: [Session/time information | Weather information | Targets]\n",
    "\n",
    "df_clean = df_clean[['SESSION_UID', 'SESSION_TIME', \n",
    "         'SESSION_TYPE', 'TIME_OFFSET', 'NUM_WEATHER_FORECAST_SAMPLES', \n",
    "         'TRACK_TEMPERATURE', 'TRACK_TEMPERATURE_CHANGE', 'AIR_TEMPERATURE', 'AIR_TEMPERATURE_CHANGE',\n",
    "         'WEATHER_FORECAST_SAMPLES_M_TRACK_TEMPERATURE', 'WEATHER_FORECAST_SAMPLES_M_AIR_TEMPERATURE', \n",
    "         'WEATHER_FORECAST_SAMPLES_M_WEATHER', 'RAIN_PERCENTAGE', 'WEATHER']]\n",
    "\n",
    "# Rename columns for readability \n",
    "df_clean = df_clean.rename(columns={'WEATHER_FORECAST_SAMPLES_M_TRACK_TEMPERATURE': 'FORECAST_TRACK_TEMP', \n",
    "                                    'WEATHER_FORECAST_SAMPLES_M_AIR_TEMPERATURE': 'FORECAST_AIR_TEMP',\n",
    "                                    'WEATHER_FORECAST_SAMPLES_M_WEATHER': 'FORECAST_WEATHER',\n",
    "                                    'NUM_WEATHER_FORECAST_SAMPLES': 'N_FORECASTS',\n",
    "                                    'TRACK_TEMPERATURE_CHANGE': 'TRACK_TEMP_CHANGE',\n",
    "                                    'AIR_TEMPERATURE_CHANGE': 'AIR_TEMP_CHANGE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea32fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign catagorical datatype to relevant columns\n",
    "df_clean['TRACK_TEMP_CHANGE'] = df_clean['TRACK_TEMP_CHANGE'].apply(np.floor)\n",
    "\n",
    "df_clean = df_clean.astype({\"WEATHER\": \"category\",\n",
    "                            \"TRACK_TEMP_CHANGE\": \"category\",\n",
    "                            \"AIR_TEMP_CHANGE\": \"category\"})\n",
    "\n",
    "# Experimentation conducted with One Hot Encoder & feature standardization.\n",
    "# This was ultimately omitted due to negligible difference seen in the prediction\n",
    "# error.\n",
    "# OHE = ce.OneHotEncoder(cols=['TRACK_TEMP_CHANGE',\n",
    "#                              'AIR_TEMP_CHANGE'], use_cat_names=True)\n",
    "\n",
    "# df_clean = OHE.fit_transform(df_clean)\n",
    "\n",
    "# df_clean[['AIR_TEMPERATURE', 'TRACK_TEMPERATURE', 'FORECAST_AIR_TEMP',\n",
    "#           'FORECAST_TRACK_TEMP']] = StandardScaler().fit_transform(df_clean[['AIR_TEMPERATURE', 'TRACK_TEMPERATURE', \n",
    "#                                                                              'FORECAST_AIR_TEMP','FORECAST_TRACK_TEMP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a5c11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish list of all sessions, start session_times and end session_times. \n",
    "# Done in order to both better understand the sessional data, and provide a list to iterate\n",
    "# through for the following steps.\n",
    "\n",
    "top = df_clean.groupby(['SESSION_UID'], sort=False)['SESSION_TIME'].max().to_frame().reset_index()\n",
    "bot = df_clean.groupby(['SESSION_UID'], sort=False)['SESSION_TIME'].min().to_frame().reset_index()\n",
    "\n",
    "times = bot.merge(top, on=['SESSION_UID'], how='left')\n",
    "\n",
    "times = times.rename(columns={'SESSION_TIME_x': 'MIN_TIME (SECS)', \n",
    "                              'SESSION_TIME_y': 'MAX_TIME (SECS)'})\n",
    "times['DURATION (MINS)'] = (times['MAX_TIME (SECS)'] - times['MIN_TIME (SECS)'])/60\n",
    "\n",
    "times = times.sort_values(by='DURATION (MINS)', ascending=False)\n",
    "\n",
    "times = times.reset_index(drop=True)\n",
    "\n",
    "#times.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596755c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13534 8164 2242 1089 667\n"
     ]
    }
   ],
   "source": [
    "# Utilise function to append true labels at specified offsets from future observations \n",
    "# of session. dependent on whether the future session time exists in the session or not\n",
    "# Observations are omitted if no true label can be discovered within the same session.\n",
    "\n",
    "data_5 = pd.DataFrame()\n",
    "data_5 = prep_df(times, df_clean, 5, data_5)\n",
    "# data_5_prec.to_csv('~/CSVs/data_5_prec.csv', index=False)\n",
    "# data_5_weather.to_csv('~/CSVs/data_5_weather.csv', index=False)\n",
    "\n",
    "data_10 = pd.DataFrame()\n",
    "data_10 = prep_df(times, df_clean, 10, data_10)\n",
    "# data_10_prec.to_csv('~/CSVs/data_10_prec.csv', index=False)\n",
    "# data_10_weather.to_csv('~/CSVs/data_10_weather.csv', index=False)\n",
    "\n",
    "data_15 = pd.DataFrame()\n",
    "data_15 = prep_df(times, df_clean, 15, data_15)\n",
    "\n",
    "data_30 = pd.DataFrame()\n",
    "data_30 = prep_df(times, df_clean, 30, data_30)\n",
    "\n",
    "data_60 = pd.DataFrame()\n",
    "data_60 = prep_df(times, df_clean, 60, data_60)\n",
    "\n",
    "print(len(data_5), len(data_10), len(data_15), len(data_30), len(data_60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f33f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 9671, 1.0: 3181, 2.0: 682})\n",
      "Counter({0.0: 6046, 1.0: 1685, 2.0: 433})\n",
      "Counter({0.0: 1911, 1.0: 331})\n",
      "Counter({0.0: 1089})\n",
      "Counter({0.0: 667}) \n",
      "\n",
      "Counter({1.0: 3410, 3.0: 2347, 4.0: 2212, 2.0: 2025, 12.0: 989, 21.0: 682, 5.0: 553, 11.0: 373, 13.0: 358, 10.0: 328, 8.0: 127, 6.0: 88, 2.5: 22, 14.0: 20})\n",
      "Counter({1.0: 2445, 4.0: 1481, 2.0: 1122, 3.0: 947, 12.0: 717, 21.0: 433, 5.0: 334, 11.0: 252, 8.0: 179, 13.0: 164, 10.0: 77, 2.5: 7, 9.0: 6})\n",
      "Counter({2.0: 938, 1.0: 795, 10.0: 261, 5.0: 107, 6.0: 62, 3.0: 58, 8.0: 15, 9.0: 6})\n",
      "Counter({2.0: 754, 1.0: 242, 5.0: 93})\n",
      "Counter({2.0: 432, 1.0: 235})\n"
     ]
    }
   ],
   "source": [
    "# Quick EDA on what degree of imbalance exists between classes and values\n",
    "\n",
    "print(Counter(data_5['PRED_WEATHER']))\n",
    "print(Counter(data_10['PRED_WEATHER']))\n",
    "print(Counter(data_15['PRED_WEATHER']))\n",
    "print(Counter(data_30['PRED_WEATHER']))\n",
    "print(Counter(data_60['PRED_WEATHER']),'\\n')\n",
    "\n",
    "print(Counter(data_5['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_10['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_15['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_30['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_60['PRED_RAIN_PERCENTAGE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538221cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 9671, 1.0: 3181, 2.0: 682})\n",
      "Counter({0.0: 6046, 1.0: 1685, 2.0: 433})\n",
      "Counter({0.0: 1911, 1.0: 331, 2.0: 51})\n",
      "Counter({0.0: 8553, 1.0: 5887, 2.0: 51})\n",
      "Counter({0.0: 8131, 1.0: 5887, 2.0: 51}) \n",
      "\n",
      "Counter({1.0: 3410, 3.0: 2347, 4.0: 2212, 2.0: 2025, 12.0: 989, 21.0: 682, 5.0: 553, 11.0: 373, 13.0: 358, 10.0: 328, 8.0: 127, 6.0: 88, 2.5: 22, 14.0: 20})\n",
      "Counter({1.0: 2445, 4.0: 1481, 2.0: 1122, 3.0: 947, 12.0: 717, 21.0: 433, 5.0: 334, 11.0: 252, 8.0: 179, 13.0: 164, 10.0: 77, 2.5: 7, 9.0: 6})\n",
      "Counter({2.0: 938, 1.0: 795, 10.0: 261, 5.0: 107, 6.0: 62, 3.0: 58, 16.0: 37, 8.0: 15, 24.0: 9, 9.0: 6, 13.0: 4, 14.0: 1})\n",
      "Counter({14.0: 5487, 4.0: 4212, 8.0: 897, 3.0: 786, 2.0: 757, 11.0: 440, 7.0: 362, 12.0: 281, 13.0: 279, 5.0: 255, 1.0: 242, 6.0: 215, 10.0: 168, 9.0: 64, 16.0: 37, 21.0: 9})\n",
      "Counter({14.0: 6450, 4.0: 4804, 8.0: 1704, 2.0: 472, 1.0: 235, 11.0: 160, 10.0: 105, 6.0: 38, 18.0: 37, 5.0: 36, 3.0: 10, 15.0: 9, 12.0: 4, 19.0: 4, 2.5: 1})\n"
     ]
    }
   ],
   "source": [
    "# For underpopulated & imbalanced offsets (namely 15, 30 and 60), a \n",
    "# solution design choice is made to utilise the forecast values for\n",
    "# weather & precipitation as a source of ground truth. The idea is\n",
    "# to introduce more diversity amongst the classes at the longer time,\n",
    "# in order to prevent likelihood of it being a single class classifier - \n",
    "# given that we already have a limited dataset with a lack of certain\n",
    "# weather types after our filtering.\n",
    "\n",
    "current = df_clean[(df_clean['TIME_OFFSET'] == 0)] \n",
    "\n",
    "offset_15 = df_clean[(df_clean['TIME_OFFSET'] == 15)]\n",
    "offset_30 = df_clean[(df_clean['TIME_OFFSET'] == 30)]\n",
    "offset_60 = df_clean[(df_clean['TIME_OFFSET'] == 60)]\n",
    "\n",
    "forecast_15 = offset_prep(offset_15, current)\n",
    "forecast_30 = offset_prep(offset_30, current)\n",
    "forecast_60 = offset_prep(offset_60, current)\n",
    "\n",
    "forecast_15 = forecast_15[forecast_15['PRED_WEATHER'] == 2]\n",
    "data_15n = pd.concat([data_15, forecast_15])\n",
    "data_30n = pd.concat([data_30, forecast_30])\n",
    "data_60n = pd.concat([data_60, forecast_60])\n",
    "\n",
    "# Quick EDA on what degree of imbalance exists between classes and values\n",
    "# following on from the appendage of forecast values as a source of ground\n",
    "# truth for offsets 15, 30 & 60.\n",
    "\n",
    "print(Counter(data_5['PRED_WEATHER']))\n",
    "print(Counter(data_10['PRED_WEATHER']))\n",
    "print(Counter(data_15n['PRED_WEATHER']))\n",
    "print(Counter(data_30n['PRED_WEATHER']))\n",
    "print(Counter(data_60n['PRED_WEATHER']),'\\n')\n",
    "\n",
    "print(Counter(data_5['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_10['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_15n['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_30n['PRED_RAIN_PERCENTAGE']))\n",
    "print(Counter(data_60n['PRED_RAIN_PERCENTAGE']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a540a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFFSET 5\n",
      "NO SMOTE: Counter({0.0: 9671, 1.0: 3181, 2.0: 682})\n",
      "\n",
      "OFFSET 10\n",
      "NO SMOTE: Counter({0.0: 6046, 1.0: 1685, 2.0: 433})\n",
      "\n",
      "OFFSET 15\n",
      "BEFORE: Counter({0.0: 1911, 1.0: 331, 2.0: 51})\n",
      "AFTER: Counter({0.0: 1911, 1.0: 1911, 2.0: 1911})\n",
      "\n",
      "OFFSET 30\n",
      "BEFORE: Counter({0.0: 8553, 1.0: 5887, 2.0: 51})\n",
      "AFTER: Counter({0.0: 8553, 1.0: 8553, 2.0: 8553})\n",
      "\n",
      "OFFSET 60\n",
      "BEFORE: Counter({0.0: 8131, 1.0: 5887, 2.0: 51})\n",
      "AFTER: Counter({0.0: 8131, 1.0: 8131, 2.0: 8131})\n"
     ]
    }
   ],
   "source": [
    "# This chunk creates a split in the dataframe for precipitation & weather models, and additionally \n",
    "# creates test and train dataframes for the respective precipitation & weather datasets.\n",
    "# Where imbalances were identified previously and addressed through the use of future forecast\n",
    "# values as a source of ground truth, we utilise SMOTE to further address the class imbalance that \n",
    "# is present in offsets 15, 30 and 60. This is all done from within the split_targets function.\n",
    "\n",
    "print('OFFSET 5')\n",
    "prec5_train_x, prec5_train_y, weath5_train_x, weath5_train_y = split_targets(data_5, smote = 0)\n",
    "prec5_train_x, prec5_test_x, prec5_train_y, prec5_test_y = train_test_split(prec5_train_x, prec5_train_y, \n",
    "                                                                        test_size=0.30,random_state=0)\n",
    "weath5_train_x, weath5_test_x, weath5_train_y, weath5_test_y = train_test_split(weath5_train_x, weath5_train_y, \n",
    "                                                                         test_size=0.30,random_state=0)\n",
    "# #######################################################################\n",
    "print('\\nOFFSET 10') \n",
    "prec10_train_x, prec10_train_y, weath10_train_x, weath10_train_y = split_targets(data_10, smote = 0)\n",
    "prec10_train_x, prec10_test_x, prec10_train_y, prec10_test_y = train_test_split(prec10_train_x, prec10_train_y, \n",
    "                                                                        test_size=0.30,random_state=0)\n",
    "weath10_train_x, weath10_test_x, weath10_train_y, weath10_test_y = train_test_split(weath10_train_x, weath10_train_y, \n",
    "                                                                         test_size=0.30,random_state=0)\n",
    "\n",
    "#######################################################################\n",
    "print('\\nOFFSET 15')\n",
    "prec15_train_x, prec15_train_y, weath15_train_x, weath15_train_y = split_targets(data_15n, smote = 1)\n",
    "prec15_train_x, prec15_test_x, prec15_train_y, prec15_test_y = train_test_split(prec15_train_x, prec15_train_y, \n",
    "                                                                        test_size=0.30,random_state=0)\n",
    "weath15_train_x, weath15_test_x, weath15_train_y, weath15_test_y = train_test_split(weath15_train_x, weath15_train_y, \n",
    "                                                                         test_size=0.30,random_state=0)\n",
    "\n",
    "#######################################################################\n",
    "print('\\nOFFSET 30') \n",
    "prec30_train_x, prec30_train_y, weath30_train_x, weath30_train_y = split_targets(data_30n, smote = 1)\n",
    "prec30_train_x, prec30_test_x, prec30_train_y, prec30_test_y = train_test_split(prec30_train_x, prec30_train_y, \n",
    "                                                                        test_size=0.30,random_state=0)\n",
    "weath30_train_x, weath30_test_x, weath30_train_y, weath30_test_y = train_test_split(weath30_train_x, weath30_train_y, \n",
    "                                                                         test_size=0.30,random_state=0)\n",
    "\n",
    "#######################################################################\n",
    "print('\\nOFFSET 60') \n",
    "prec60_train_x, prec60_train_y, weath60_train_x, weath60_train_y = split_targets(data_60n, smote = 1)\n",
    "prec60_train_x, prec60_test_x, prec60_train_y, prec60_test_y = train_test_split(prec60_train_x, prec60_train_y, \n",
    "                                                                        test_size=0.30,random_state=0)\n",
    "weath60_train_x, weath60_test_x, weath60_train_y, weath60_test_y = train_test_split(weath60_train_x, weath60_train_y, \n",
    "                                                                         test_size=0.30,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "467ec379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model exploration/testing.\n",
    "\n",
    "# model_LR = LinearRegression()\n",
    "\n",
    "# # fit the model with the training data\n",
    "# model_LR.fit(prec5_train_x, prec5_train_y)\n",
    "\n",
    "# predict_train = model_LR.predict(prec5_train_x)\n",
    "# predict_test  = model_LR.predict(prec5_test_x)\n",
    "\n",
    "# print('RMSE on train data: ', mean_squared_error(prec5_train_y, predict_train)**(0.5))\n",
    "# print('RMSE on test data: ',  mean_squared_error(prec5_test_y, predict_test)**(0.5))\n",
    "\n",
    "# print('MAE on train data: ', mean_absolute_error(prec5_train_y, predict_train))\n",
    "# print('MAE on test data: ', mean_absolute_error(prec5_test_y, predict_test))\n",
    "\n",
    "# model_LR = LinearRegression()\n",
    "\n",
    "# # fit the model with the training data\n",
    "# model_LR.fit(weath5_train_x, weath5_train_y)\n",
    "\n",
    "# predict_train = model_LR.predict(weath5_train_x)\n",
    "# predict_test  = model_LR.predict(weath5_test_x)\n",
    "\n",
    "# print('RMSE on train data: ', mean_squared_error(weath5_train_y, predict_train)**(0.5))\n",
    "# print('RMSE on test data: ',  mean_squared_error(weath5_test_y, predict_test)**(0.5))\n",
    "\n",
    "# print('MAE on train data: ', mean_absolute_error(weath5_train_y, predict_train))\n",
    "# print('MAE on test data: ', mean_absolute_error(weath5_test_y, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc9d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:2.71664\ttest-mae:2.77235\n",
      "[1]\ttrain-mae:1.83776\ttest-mae:1.86492\n",
      "[2]\ttrain-mae:1.33850\ttest-mae:1.34981\n",
      "[3]\ttrain-mae:1.05701\ttest-mae:1.06011\n",
      "[4]\ttrain-mae:0.88119\ttest-mae:0.87769\n",
      "[5]\ttrain-mae:0.78515\ttest-mae:0.77815\n",
      "[6]\ttrain-mae:0.71559\ttest-mae:0.70629\n",
      "[7]\ttrain-mae:0.67182\ttest-mae:0.66054\n",
      "[8]\ttrain-mae:0.64169\ttest-mae:0.62837\n",
      "[9]\ttrain-mae:0.60996\ttest-mae:0.59861\n",
      "\n",
      "[0]\ttrain-mae:0.27494\ttest-mae:0.27624\n",
      "[1]\ttrain-mae:0.13753\ttest-mae:0.13819\n",
      "[2]\ttrain-mae:0.06880\ttest-mae:0.06913\n",
      "[3]\ttrain-mae:0.03442\ttest-mae:0.03458\n",
      "[4]\ttrain-mae:0.01722\ttest-mae:0.01730\n",
      "[5]\ttrain-mae:0.00861\ttest-mae:0.00865\n",
      "[6]\ttrain-mae:0.00431\ttest-mae:0.00433\n",
      "[7]\ttrain-mae:0.00215\ttest-mae:0.00217\n",
      "[8]\ttrain-mae:0.00108\ttest-mae:0.00108\n",
      "[9]\ttrain-mae:0.00054\ttest-mae:0.00054\n"
     ]
    }
   ],
   "source": [
    "# Creation of models for different offset values\n",
    "\n",
    "############################ OFFSET 5 MODELLING ############################\n",
    "\n",
    "dmat_train = xgb.DMatrix(prec5_train_x, prec5_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(prec5_test_x, prec5_test_y, enable_categorical=True)\n",
    "\n",
    "prec5_mdl = xgb.train({'max_depth': 3, 'eta': 0.4, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "prec5_resids = pd.DataFrame({ \"Actuals\":prec5_test_y, \"Prediction\":prec5_mdl.predict(dmat_test)})\n",
    "print()\n",
    "\n",
    "dmat_train = xgb.DMatrix(weath5_train_x, weath5_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(weath5_test_x, weath5_test_y, enable_categorical=True)\n",
    "\n",
    "weath5_mdl = xgb.train({'max_depth': 3, 'eta': 0.5, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "pickle.dump(prec5_mdl, open(\"prec5_mdl.sav\", 'wb'))\n",
    "pickle.dump(weath5_mdl, open(\"weath5_mdl.sav\", 'wb'))\n",
    "\n",
    "weath5_resids = pd.DataFrame({ \"Actuals\":weath5_test_y, \"Prediction\":weath5_mdl.predict(dmat_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0480e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:2.76903\ttest-mae:2.79713\n",
      "[1]\ttrain-mae:1.88672\ttest-mae:1.90574\n",
      "[2]\ttrain-mae:1.40521\ttest-mae:1.41930\n",
      "[3]\ttrain-mae:1.13893\ttest-mae:1.14904\n",
      "[4]\ttrain-mae:0.97617\ttest-mae:0.99171\n",
      "[5]\ttrain-mae:0.87806\ttest-mae:0.88908\n",
      "[6]\ttrain-mae:0.78356\ttest-mae:0.78373\n",
      "[7]\ttrain-mae:0.74007\ttest-mae:0.73628\n",
      "[8]\ttrain-mae:0.70285\ttest-mae:0.70306\n",
      "[9]\ttrain-mae:0.68964\ttest-mae:0.69017\n",
      "\n",
      "[0]\ttrain-mae:0.28197\ttest-mae:0.28195\n",
      "[1]\ttrain-mae:0.14511\ttest-mae:0.14459\n",
      "[2]\ttrain-mae:0.07659\ttest-mae:0.07583\n",
      "[3]\ttrain-mae:0.03997\ttest-mae:0.03941\n",
      "[4]\ttrain-mae:0.02834\ttest-mae:0.02838\n",
      "[5]\ttrain-mae:0.01635\ttest-mae:0.01677\n",
      "[6]\ttrain-mae:0.00881\ttest-mae:0.00911\n",
      "[7]\ttrain-mae:0.00442\ttest-mae:0.00460\n",
      "[8]\ttrain-mae:0.00342\ttest-mae:0.00361\n",
      "[9]\ttrain-mae:0.00182\ttest-mae:0.00193\n"
     ]
    }
   ],
   "source": [
    "############################ OFFSET 10 MODELLING ############################\n",
    "\n",
    "dmat_train = xgb.DMatrix(prec10_train_x, prec10_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(prec10_test_x, prec10_test_y, enable_categorical=True)\n",
    "\n",
    "prec10_mdl = xgb.train({'max_depth': 3, 'eta': 0.4, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "prec10_resids = pd.DataFrame({ \"Actuals\":prec10_test_y, \"Prediction\":prec10_mdl.predict(dmat_test)})\n",
    "print()\n",
    "\n",
    "dmat_train = xgb.DMatrix(weath10_train_x, weath10_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(weath10_test_x, weath10_test_y, enable_categorical=True)\n",
    "\n",
    "weath10_mdl = xgb.train({'max_depth': 3, 'eta': 0.5, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "pickle.dump(prec10_mdl, open(\"prec10_mdl.sav\", 'wb'))\n",
    "pickle.dump(weath10_mdl, open(\"weath10_mdl.sav\", 'wb'))\n",
    "\n",
    "weath10_resids = pd.DataFrame({ \"Actuals\":weath10_test_y, \"Prediction\":weath10_mdl.predict(dmat_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e2cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:1.61809\ttest-mae:1.73328\n",
      "[1]\ttrain-mae:0.98818\ttest-mae:1.05580\n",
      "[2]\ttrain-mae:0.62101\ttest-mae:0.66651\n",
      "[3]\ttrain-mae:0.41499\ttest-mae:0.45058\n",
      "[4]\ttrain-mae:0.30380\ttest-mae:0.33017\n",
      "[5]\ttrain-mae:0.20482\ttest-mae:0.22915\n",
      "[6]\ttrain-mae:0.15499\ttest-mae:0.17751\n",
      "[7]\ttrain-mae:0.11985\ttest-mae:0.13808\n",
      "[8]\ttrain-mae:0.08909\ttest-mae:0.10684\n",
      "[9]\ttrain-mae:0.06812\ttest-mae:0.08619\n",
      "\n",
      "[0]\ttrain-mae:0.41926\ttest-mae:0.41193\n",
      "[1]\ttrain-mae:0.20979\ttest-mae:0.20641\n",
      "[2]\ttrain-mae:0.10497\ttest-mae:0.10357\n",
      "[3]\ttrain-mae:0.05253\ttest-mae:0.05212\n",
      "[4]\ttrain-mae:0.02628\ttest-mae:0.02637\n",
      "[5]\ttrain-mae:0.01315\ttest-mae:0.01348\n",
      "[6]\ttrain-mae:0.00658\ttest-mae:0.00704\n",
      "[7]\ttrain-mae:0.00329\ttest-mae:0.00381\n",
      "[8]\ttrain-mae:0.00165\ttest-mae:0.00220\n",
      "[9]\ttrain-mae:0.00082\ttest-mae:0.00139\n"
     ]
    }
   ],
   "source": [
    "############################ OFFSET 15 MODELLING ############################\n",
    "\n",
    "dmat_train = xgb.DMatrix(prec15_train_x, prec15_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(prec15_test_x, prec15_test_y, enable_categorical=True)\n",
    "\n",
    "prec15_mdl = xgb.train({'max_depth': 3, 'eta': 0.4, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "prec15_resids = pd.DataFrame({ \"Actuals\":prec15_test_y, \"Prediction\":prec15_mdl.predict(dmat_test)})\n",
    "print()\n",
    "\n",
    "dmat_train = xgb.DMatrix(weath15_train_x, weath15_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(weath15_test_x, weath15_test_y, enable_categorical=True)\n",
    "\n",
    "weath15_mdl = xgb.train({'max_depth': 3, 'eta': 0.5, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "pickle.dump(prec15_mdl, open(\"prec15_mdl.sav\", 'wb'))\n",
    "pickle.dump(weath15_mdl, open(\"weath15_mdl.sav\", 'wb'))\n",
    "\n",
    "weath15_resids = pd.DataFrame({ \"Actuals\":weath15_test_y, \"Prediction\":weath15_mdl.predict(dmat_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3be8cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:4.89518\ttest-mae:4.83910\n",
      "[1]\ttrain-mae:2.95365\ttest-mae:2.91768\n",
      "[2]\ttrain-mae:1.81672\ttest-mae:1.79528\n",
      "[3]\ttrain-mae:1.13046\ttest-mae:1.11807\n",
      "[4]\ttrain-mae:0.72406\ttest-mae:0.71664\n",
      "[5]\ttrain-mae:0.46872\ttest-mae:0.46384\n",
      "[6]\ttrain-mae:0.32267\ttest-mae:0.32022\n",
      "[7]\ttrain-mae:0.23867\ttest-mae:0.23632\n",
      "[8]\ttrain-mae:0.17014\ttest-mae:0.16895\n",
      "[9]\ttrain-mae:0.13470\ttest-mae:0.13450\n",
      "\n",
      "[0]\ttrain-mae:0.41670\ttest-mae:0.41688\n",
      "[1]\ttrain-mae:0.20838\ttest-mae:0.20848\n",
      "[2]\ttrain-mae:0.10421\ttest-mae:0.10426\n",
      "[3]\ttrain-mae:0.05212\ttest-mae:0.05214\n",
      "[4]\ttrain-mae:0.02606\ttest-mae:0.02608\n",
      "[5]\ttrain-mae:0.01303\ttest-mae:0.01304\n",
      "[6]\ttrain-mae:0.00652\ttest-mae:0.00652\n",
      "[7]\ttrain-mae:0.00326\ttest-mae:0.00326\n",
      "[8]\ttrain-mae:0.00163\ttest-mae:0.00163\n",
      "[9]\ttrain-mae:0.00081\ttest-mae:0.00082\n"
     ]
    }
   ],
   "source": [
    "############################ OFFSET 30 MODELLING ############################\n",
    "\n",
    "dmat_train = xgb.DMatrix(prec30_train_x, prec30_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(prec30_test_x, prec30_test_y, enable_categorical=True)\n",
    "\n",
    "prec30_mdl = xgb.train({'max_depth': 3, 'eta': 0.4, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "prec30_resids = pd.DataFrame({ \"Actuals\":prec30_test_y, \"Prediction\":prec30_mdl.predict(dmat_test)})\n",
    "print()\n",
    "\n",
    "dmat_train = xgb.DMatrix(weath30_train_x, weath30_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(weath30_test_x, weath30_test_y, enable_categorical=True)\n",
    "\n",
    "weath30_mdl = xgb.train({'max_depth': 3, 'eta': 0.5, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "pickle.dump(prec30_mdl, open(\"prec30_mdl.sav\", 'wb'))\n",
    "pickle.dump(weath30_mdl, open(\"weath30_mdl.sav\", 'wb'))\n",
    "\n",
    "weath30_resids = pd.DataFrame({ \"Actuals\":weath30_test_y, \"Prediction\":weath30_mdl.predict(dmat_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c565af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:5.20665\ttest-mae:5.12806\n",
      "[1]\ttrain-mae:3.12866\ttest-mae:3.08196\n",
      "[2]\ttrain-mae:1.88999\ttest-mae:1.86295\n",
      "[3]\ttrain-mae:1.14673\ttest-mae:1.13122\n",
      "[4]\ttrain-mae:0.69452\ttest-mae:0.68486\n",
      "[5]\ttrain-mae:0.42435\ttest-mae:0.41643\n",
      "[6]\ttrain-mae:0.26258\ttest-mae:0.25686\n",
      "[7]\ttrain-mae:0.17917\ttest-mae:0.17498\n",
      "[8]\ttrain-mae:0.11722\ttest-mae:0.11423\n",
      "[9]\ttrain-mae:0.07484\ttest-mae:0.07249\n",
      "\n",
      "[0]\ttrain-mae:0.41597\ttest-mae:0.41858\n",
      "[1]\ttrain-mae:0.20803\ttest-mae:0.20933\n",
      "[2]\ttrain-mae:0.10404\ttest-mae:0.10469\n",
      "[3]\ttrain-mae:0.05203\ttest-mae:0.05236\n",
      "[4]\ttrain-mae:0.02602\ttest-mae:0.02618\n",
      "[5]\ttrain-mae:0.01301\ttest-mae:0.01310\n",
      "[6]\ttrain-mae:0.00651\ttest-mae:0.00655\n",
      "[7]\ttrain-mae:0.00326\ttest-mae:0.00328\n",
      "[8]\ttrain-mae:0.00163\ttest-mae:0.00164\n",
      "[9]\ttrain-mae:0.00081\ttest-mae:0.00082\n"
     ]
    }
   ],
   "source": [
    "############################ OFFSET 60 MODELLING ############################\n",
    "\n",
    "dmat_train = xgb.DMatrix(prec60_train_x, prec60_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(prec60_test_x, prec60_test_y, enable_categorical=True)\n",
    "\n",
    "prec60_mdl = xgb.train({'max_depth': 3, 'eta': 0.4, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "prec60_resids = pd.DataFrame({ \"Actuals\":prec60_test_y, \"Prediction\":prec60_mdl.predict(dmat_test)})\n",
    "print()\n",
    "\n",
    "dmat_train = xgb.DMatrix(weath60_train_x, weath60_train_y, enable_categorical=True)\n",
    "dmat_test = xgb.DMatrix(weath60_test_x, weath60_test_y, enable_categorical=True)\n",
    "\n",
    "weath60_mdl = xgb.train({'max_depth': 3, 'eta': 0.5, 'objective': 'reg:squarederror', \"eval_metric\": \"mae\"},\n",
    "                     dmat_train, evals=[(dmat_train, \"train\"), (dmat_test, \"test\")])\n",
    "\n",
    "pickle.dump(prec60_mdl, open(\"prec60_mdl.sav\", 'wb'))\n",
    "pickle.dump(weath60_mdl, open(\"weath60_mdl.sav\", 'wb'))\n",
    "\n",
    "weath60_resids = pd.DataFrame({ \"Actuals\":weath60_test_y, \"Prediction\":weath60_mdl.predict(dmat_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efbe96db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actuals</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7318 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actuals  Prediction\n",
       "0       2.000       1.999\n",
       "1       2.000       1.999\n",
       "2       1.000       1.000\n",
       "3       1.000       1.000\n",
       "4       0.000       0.000\n",
       "...       ...         ...\n",
       "7313    2.000       1.999\n",
       "7314    0.000       0.000\n",
       "7315    0.000       0.000\n",
       "7316    0.000       0.000\n",
       "7317    0.000       0.000\n",
       "\n",
       "[7318 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BUILD RESIDUALS TABLE / PLOTS\n",
    "\n",
    "prec5_resids\n",
    "weath5_resids\n",
    "\n",
    "prec10_resids\n",
    "weath10_resids \n",
    "\n",
    "prec15_resids\n",
    "weath15_resids \n",
    "\n",
    "prec30_resids\n",
    "weath30_resids \n",
    "\n",
    "prec60_resids\n",
    "weath60_resids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cdd905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## MODEL SUBMISSION TESTING ##########################\n",
    "# The proposed models are built on the assumption that the time offset data is \n",
    "# built into the dataset. That is to say, for a prediction to be made, the session\n",
    "# duration MUST be scheduled to last for a minimum of five minutes. Thus, the \n",
    "# game will populate values for M_OFSET_TIME beyond 0, depending on the selected\n",
    "# session duration.\n",
    "\n",
    "# The predictions are made with session duration in mind. The prediction pipeline\n",
    "# will utilise any offsets between 0 and 60 to populate the forecast fields for \n",
    "# different offsets. If certain offsets are unavailable due to the chosen length\n",
    "# of the session, the offset will be populated with the largest offset values \n",
    "# available (if offset 60 is unavailable, the model for offset 60 will populate\n",
    "# with forecast values from offset 30, and so on)\n",
    "\n",
    "# Please insert desired .csv containing data regarding the prediction. Currently \n",
    "# filled with dummy dataset \"weather-Copy1.csv\"\n",
    "\n",
    "df2 = pd.read_csv('weather-Copy1.csv', index_col=False, low_memory=False)\n",
    "\n",
    "prec5_mdl =  pickle.load(open(\"prec5_mdl.sav\", 'rb'))\n",
    "weath5_mdl = pickle.load(open(\"weath5_mdl.sav\", 'rb'))\n",
    "prec10_mdl = pickle.load(open(\"prec10_mdl.sav\", 'rb'))\n",
    "weath10_mdl =pickle.load(open(\"weath10_mdl.sav\", 'rb'))\n",
    "prec15_mdl = pickle.load(open(\"prec15_mdl.sav\", 'rb'))\n",
    "weath15_mdl =pickle.load(open(\"weath15_mdl.sav\", 'rb'))\n",
    "prec30_mdl = pickle.load(open(\"prec30_mdl.sav\", 'rb'))\n",
    "weath30_mdl =pickle.load(open(\"weath30_mdl.sav\", 'rb'))\n",
    "prec60_mdl = pickle.load(open(\"prec60_mdl.sav\", 'rb'))\n",
    "weath60_mdl =pickle.load(open(\"weath60_mdl.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27842ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2\n",
    "\n",
    "# Rename columns for ease of readability & for compatibility with previous\n",
    "# model pipeline\n",
    "df3.columns = df3.columns.str.replace(r'^M_', '', regex=True)\n",
    "\n",
    "# Drop columns which are not relevant for the model\n",
    "df3 = df3.drop(columns=['PACKET_FORMAT', 'GAME_MAJOR_VERSION', 'PACKET_VERSION', \n",
    "                        'PACKET_ID', 'SECONDARY_PLAYER_CAR_INDEX', 'SLI_PRO_NATIVE_SUPPORT', \n",
    "                        'SAFETY_CAR_STATUS', 'DRSASSIST', 'STEERING_ASSIST', 'AI_DIFFICULTY', \n",
    "                        'NETWORK_GAME', 'PIT_RELEASE_ASSIST','BRAKING_ASSIST', \n",
    "                        'GAME_MINOR_VERSION', 'ERSASSIST', 'PIT_ASSIST', 'GEARBOX_ASSIST',\n",
    "                        'DYNAMIC_RACING_LINE', 'DYNAMIC_RACING_LINE_TYPE', 'PIT_SPEED_LIMIT', \n",
    "                        'SPECTATOR_CAR_INDEX', 'FRAME_IDENTIFIER', 'GAMEHOST', 'ZONE_START', \n",
    "                        'ZONE_FLAG', 'PIT_STOP_REJOIN_POSITION', 'NUM_MARSHAL_ZONES', \n",
    "                        'SEASON_LINK_IDENTIFIER', 'WEEKEND_LINK_IDENTIFIER', \n",
    "                        'SESSION_LINK_IDENTIFIER','PIT_STOP_WINDOW_LATEST_LAP', \n",
    "                        'Unnamed: 58', 'TIMESTAMP','FORMULA', 'PLAYER_CAR_INDEX', \n",
    "                        'TOTAL_LAPS', 'TRACK_LENGTH', 'PIT_STOP_WINDOW_IDEAL_LAP', \n",
    "                        'GAME_PAUSED', 'FORECAST_ACCURACY', 'TRACK_ID', 'SESSION_TYPE', \n",
    "                        'WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE', 'SESSION_DURATION', \n",
    "                        'IS_SPECTATING', 'SESSION_TIME_LEFT']) \n",
    "\n",
    "# Rename columns further for ease of readability & for compatibility with previous model \n",
    "# pipeline\n",
    "df3 = df3.rename(columns={'WEATHER_FORECAST_SAMPLES_M_TRACK_TEMPERATURE': 'FORECAST_TRACK_TEMP', \n",
    "                            'WEATHER_FORECAST_SAMPLES_M_AIR_TEMPERATURE': 'FORECAST_AIR_TEMP',\n",
    "                            'WEATHER_FORECAST_SAMPLES_M_WEATHER': 'FORECAST_WEATHER',\n",
    "                            'NUM_WEATHER_FORECAST_SAMPLES': 'N_FORECASTS',\n",
    "                            'TRACK_TEMPERATURE_CHANGE': 'TRACK_TEMP_CHANGE',\n",
    "                            'AIR_TEMPERATURE_CHANGE': 'AIR_TEMP_CHANGE'})\n",
    "\n",
    "#df3 = df3[df3['N_FORECASTS'] > 0]\n",
    "df3 = df3.drop(columns='N_FORECASTS')\n",
    "\n",
    "# Reorder dataset [done for QA purposes]\n",
    "df3 = df3[['SESSION_UID', 'SESSION_TIME', 'TIME_OFFSET', 'TRACK_TEMPERATURE', \n",
    "           'TRACK_TEMP_CHANGE', 'AIR_TEMPERATURE', 'AIR_TEMP_CHANGE', \n",
    "           'FORECAST_TRACK_TEMP', 'FORECAST_AIR_TEMP', 'FORECAST_WEATHER', \n",
    "           'RAIN_PERCENTAGE', 'WEATHER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8cc3897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Length is incompatible (<5 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Takes session ID (assuming only one session ID is present in the dataset) - but\n",
    "# can take alternatives below\n",
    "target_session = df3['SESSION_UID'].unique() \n",
    "\n",
    "# takes first ranked session ID (can be modified as required) if other sessions are \n",
    "# desired\n",
    "target_df = df3[df3['SESSION_UID'] == target_session[0]] \n",
    "\n",
    "# Locate offsets between 0 and 60 in the provided data, according to the target session\n",
    "target_df = target_df[(target_df['TIME_OFFSET'] == 0) | (target_df['TIME_OFFSET'] == 5) | \n",
    "                      (target_df['TIME_OFFSET'] == 10) | (target_df['TIME_OFFSET'] == 15) |\n",
    "                      (target_df['TIME_OFFSET'] == 30) | (target_df['TIME_OFFSET'] == 60)]\n",
    "\n",
    "# Locate target/initial second in the target session \n",
    "target_df = target_df.sort_values(by=['SESSION_TIME'])\n",
    "target_sec = target_df[target_df['SESSION_TIME'] == target_df['SESSION_TIME'].iloc[0]]\n",
    "\n",
    "# Identify number of offsets available to the session - required for logic related to\n",
    "# prediction pipeline\n",
    "n_horizon = target_sec['TIME_OFFSET'].nunique() \n",
    "\n",
    "# Localize top of session and truncate prediction dataframe to isolate the rows\n",
    "# relating to the target session/second\n",
    "target = target_sec.head(n_horizon).sort_values(by=['TIME_OFFSET'])\n",
    "\n",
    "# Logic for prediction pipeline. Please see explanation provided 2 chunks above.\n",
    "# forecast values are populated from either the respective offset observation,\n",
    "# or from the largest offset observation values available.\n",
    "if n_horizon == 6:\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[5]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[5]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "elif n_horizon == 5:\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "elif n_horizon == 4:\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "elif n_horizon == 3:\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "elif n_horizon == 2:\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "elif n_horizon == 1:\n",
    "    print('Session Length is incompatible (<5 minutes)')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27479f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: {'type': 0, 'rain_percentage': 2.4603262},\n",
       " 10: {'type': 1, 'rain_percentage': 5.6672416},\n",
       " 15: {'type': 2, 'rain_percentage': 15.719559},\n",
       " 30: {'type': 0, 'rain_percentage': 3.7601113},\n",
       " 60: {'type': 0, 'rain_percentage': 5.0989795}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = { 5: {'type': weath5_pred,  'rain_percentage': prec5_pred[0]},\n",
    "          10: {'type': weath10_pred, 'rain_percentage': prec10_pred[0]},\n",
    "          15: {'type': weath15_pred, 'rain_percentage': prec15_pred[0]},\n",
    "          30: {'type': weath30_pred, 'rain_percentage': prec30_pred[0]},\n",
    "          60: {'type': weath60_pred, 'rain_percentage': prec60_pred[0]}}\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalmachinelearningforcpusonpython3_7vy]",
   "language": "python",
   "name": "conda-env-generalmachinelearningforcpusonpython3_7vy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
