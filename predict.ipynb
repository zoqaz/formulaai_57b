{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db79421f",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a637af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HACKMAKERS FORMULA AI HACKATHON 2022\n",
    "\n",
    "### JUPYTER NOTEBOOK SUBMISSION FOR TEAM 57B:\n",
    "### ALEC ZHANG, DAVID ZHOU, DILAN DE SILVA,\n",
    "### THOMAS CRAWLEY, ZOHAIB QAZI\n",
    "\n",
    "### NOTEBOOK IS CREATED FOR CONDUCTING PREDICTIONS\n",
    "### ALSO FOUND IN FINAL 3 CHUNKS OF Untitled.ipynb\n",
    "\n",
    "# Import dependencies & set display options\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f70cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## MODEL SUBMISSION TESTING ##########################\n",
    "# The proposed models are built on the assumption that the time offset data is \n",
    "# built into the dataset. That is to say, for a prediction to be made, the session\n",
    "# duration MUST be scheduled to last for a minimum of five minutes. Thus, the \n",
    "# game will populate values for M_OFSET_TIME beyond 0, depending on the selected\n",
    "# session duration.\n",
    "\n",
    "# The predictions are made with session duration in mind. The prediction pipeline\n",
    "# will utilise any offsets between 0 and 60 to populate the forecast fields for \n",
    "# different offsets. If certain offsets are unavailable due to the chosen length\n",
    "# of the session, the offset will be populated with the largest offset values \n",
    "# available (if offset 60 is unavailable, the model for offset 60 will populate\n",
    "# with forecast values from offset 30, and so on)\n",
    "\n",
    "# Please insert desired .csv containing data regarding the prediction. Currently \n",
    "# filled with dummy dataset \"weather-Copy1.csv\"\n",
    "\n",
    "df2 = pd.read_csv('~/weather-Copy1.csv', index_col=False, low_memory=False)\n",
    "\n",
    "# Load all pickled models for different offset horizons and labels \n",
    "prec5_mdl =  pickle.load(open(\"models/prec5_mdl.sav\", 'rb'))\n",
    "weath5_mdl = pickle.load(open(\"models/weath5_mdl.sav\", 'rb'))\n",
    "prec10_mdl = pickle.load(open(\"models/prec10_mdl.sav\", 'rb'))\n",
    "weath10_mdl =pickle.load(open(\"models/weath10_mdl.sav\", 'rb'))\n",
    "prec15_mdl = pickle.load(open(\"models/prec15_mdl.sav\", 'rb'))\n",
    "weath15_mdl =pickle.load(open(\"models/weath15_mdl.sav\", 'rb'))\n",
    "prec30_mdl = pickle.load(open(\"models/prec30_mdl.sav\", 'rb'))\n",
    "weath30_mdl =pickle.load(open(\"models/weath30_mdl.sav\", 'rb'))\n",
    "prec60_mdl = pickle.load(open(\"models/prec60_mdl.sav\", 'rb'))\n",
    "weath60_mdl =pickle.load(open(\"models/weath60_mdl.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00dedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2\n",
    "\n",
    "# Rename columns for ease of readability & for compatibility with previous\n",
    "# model pipeline\n",
    "df3.columns = df3.columns.str.replace(r'^M_', '', regex=True)\n",
    "\n",
    "# Drop columns which are not relevant for the model\n",
    "df3 = df3.drop(columns=['PACKET_FORMAT', 'GAME_MAJOR_VERSION', 'PACKET_VERSION', \n",
    "                        'PACKET_ID', 'SECONDARY_PLAYER_CAR_INDEX', 'SLI_PRO_NATIVE_SUPPORT', \n",
    "                        'SAFETY_CAR_STATUS', 'DRSASSIST', 'STEERING_ASSIST', 'AI_DIFFICULTY', \n",
    "                        'NETWORK_GAME', 'PIT_RELEASE_ASSIST','BRAKING_ASSIST', \n",
    "                        'GAME_MINOR_VERSION', 'ERSASSIST', 'PIT_ASSIST', 'GEARBOX_ASSIST',\n",
    "                        'DYNAMIC_RACING_LINE', 'DYNAMIC_RACING_LINE_TYPE', 'PIT_SPEED_LIMIT', \n",
    "                        'SPECTATOR_CAR_INDEX', 'FRAME_IDENTIFIER', 'GAMEHOST', 'ZONE_START', \n",
    "                        'ZONE_FLAG', 'PIT_STOP_REJOIN_POSITION', 'NUM_MARSHAL_ZONES', \n",
    "                        'SEASON_LINK_IDENTIFIER', 'WEEKEND_LINK_IDENTIFIER', \n",
    "                        'SESSION_LINK_IDENTIFIER','PIT_STOP_WINDOW_LATEST_LAP', \n",
    "                        'Unnamed: 58', 'TIMESTAMP','FORMULA', 'PLAYER_CAR_INDEX', \n",
    "                        'TOTAL_LAPS', 'TRACK_LENGTH', 'PIT_STOP_WINDOW_IDEAL_LAP', \n",
    "                        'GAME_PAUSED', 'FORECAST_ACCURACY', 'TRACK_ID', 'SESSION_TYPE', \n",
    "                        'WEATHER_FORECAST_SAMPLES_M_SESSION_TYPE', 'SESSION_DURATION', \n",
    "                        'IS_SPECTATING', 'SESSION_TIME_LEFT']) \n",
    "\n",
    "# Rename columns further for ease of readability & for compatibility with previous model \n",
    "# pipeline\n",
    "df3 = df3.rename(columns={'WEATHER_FORECAST_SAMPLES_M_TRACK_TEMPERATURE': 'FORECAST_TRACK_TEMP', \n",
    "                            'WEATHER_FORECAST_SAMPLES_M_AIR_TEMPERATURE': 'FORECAST_AIR_TEMP',\n",
    "                            'WEATHER_FORECAST_SAMPLES_M_WEATHER': 'FORECAST_WEATHER',\n",
    "                            'NUM_WEATHER_FORECAST_SAMPLES': 'N_FORECASTS',\n",
    "                            'TRACK_TEMPERATURE_CHANGE': 'TRACK_TEMP_CHANGE',\n",
    "                            'AIR_TEMPERATURE_CHANGE': 'AIR_TEMP_CHANGE'})\n",
    "\n",
    "#df3 = df3[df3['N_FORECASTS'] > 0]\n",
    "df3 = df3.drop(columns='N_FORECASTS')\n",
    "\n",
    "# Reorder dataset [done for QA purposes]\n",
    "df3 = df3[['SESSION_UID', 'SESSION_TIME', 'TIME_OFFSET', 'TRACK_TEMPERATURE', \n",
    "           'TRACK_TEMP_CHANGE', 'AIR_TEMPERATURE', 'AIR_TEMP_CHANGE', \n",
    "           'FORECAST_TRACK_TEMP', 'FORECAST_AIR_TEMP', 'FORECAST_WEATHER', \n",
    "           'RAIN_PERCENTAGE', 'WEATHER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40478380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Length is <5 minutes in duration\n",
      "Confidence for all predictions is reduced\n"
     ]
    }
   ],
   "source": [
    "# Takes session ID (assuming only one session ID is present in the dataset) - but\n",
    "# can take alternatives below\n",
    "target_session = df3['SESSION_UID'].unique() \n",
    "\n",
    "# takes first ranked session ID (can be modified as required) if other sessions are \n",
    "# desired\n",
    "target_df = df3[df3['SESSION_UID'] == target_session[0]] \n",
    "\n",
    "# Locate offsets between 0 and 60 in the provided data, according to the target session\n",
    "target_df = target_df[(target_df['TIME_OFFSET'] == 0) | (target_df['TIME_OFFSET'] == 5) | \n",
    "                      (target_df['TIME_OFFSET'] == 10) | (target_df['TIME_OFFSET'] == 15) |\n",
    "                      (target_df['TIME_OFFSET'] == 30) | (target_df['TIME_OFFSET'] == 60)]\n",
    "\n",
    "# Locate target/initial second in the target session \n",
    "target_df = target_df.sort_values(by=['SESSION_TIME'])\n",
    "target_sec = target_df[target_df['SESSION_TIME'] == target_df['SESSION_TIME'].iloc[0]]\n",
    "\n",
    "# Identify number of offsets available to the session - required for logic related to\n",
    "# prediction pipeline\n",
    "n_horizon = target_sec['TIME_OFFSET'].nunique() \n",
    "\n",
    "# Localize top of session and truncate prediction dataframe to isolate the rows\n",
    "# relating to the target session/second\n",
    "target = target_sec.head(n_horizon).sort_values(by=['TIME_OFFSET'])\n",
    "\n",
    "# Logic for prediction pipeline. Please see explanation provided 2 chunks above.\n",
    "# forecast values are populated from either the respective offset observation,\n",
    "# or from the largest offset observation values available.\n",
    "if n_horizon == 6:\n",
    "    print('Session Length is >= 60 minutes in duration')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[5]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[5]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "    \n",
    "elif n_horizon == 5:\n",
    "    print('Session Length is 30-60 minutes in duration')\n",
    "    print('Confidence for the 60 minute prediction is reduced')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[4]]))\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "    \n",
    "elif n_horizon == 4:\n",
    "    print('Session Length is 15-30 minutes in duration')\n",
    "    print('Confidence for the 30 & 60 minutes prediction is reduced')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[3]]))\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "    \n",
    "elif n_horizon == 3:\n",
    "    print('Session Length is 10-15 minutes in duration')\n",
    "    print('Confidence for the 15, 30 and 60 minute predictions is reduced')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[2]]))\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "    \n",
    "elif n_horizon == 2:\n",
    "    print('Session Length is 5-10 minutes in duration')\n",
    "    print('Confidence for the predictions over 5 minutes is reduced')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[1]]))\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n",
    "    \n",
    "elif n_horizon == 1:\n",
    "    print('Session Length is <5 minutes in duration')\n",
    "    print('Confidence for all predictions is reduced')\n",
    "    weath5_pred = weath5_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec5_pred = prec5_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath10_pred = weath10_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec10_pred = prec10_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath15_pred = weath15_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec15_pred = prec15_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath30_pred = weath30_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec30_pred = prec30_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath60_pred = weath60_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    prec60_pred = prec60_mdl.predict(xgb.DMatrix(target.iloc[[0]]))\n",
    "    weath5_pred =  round(weath5_pred[0])\n",
    "    weath10_pred = round(weath10_pred[0])\n",
    "    weath15_pred = round(weath15_pred[0])\n",
    "    weath30_pred = round(weath30_pred[0])\n",
    "    weath60_pred = round(weath60_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4fd384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: {'type': 0, 'rain_percentage': 2.4603262},\n",
       " 10: {'type': 1, 'rain_percentage': 5.6672416},\n",
       " 15: {'type': 2, 'rain_percentage': 15.719559},\n",
       " 30: {'type': 0, 'rain_percentage': 3.7601113},\n",
       " 60: {'type': 0, 'rain_percentage': 5.0989795}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build output dictionary for prediction as outlined in documentation\n",
    "output = { 5: {'type': weath5_pred,  'rain_percentage': prec5_pred[0]},\n",
    "          10: {'type': weath10_pred, 'rain_percentage': prec10_pred[0]},\n",
    "          15: {'type': weath15_pred, 'rain_percentage': prec15_pred[0]},\n",
    "          30: {'type': weath30_pred, 'rain_percentage': prec30_pred[0]},\n",
    "          60: {'type': weath60_pred, 'rain_percentage': prec60_pred[0]}}\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalmachinelearningforcpusonpython3_7vy]",
   "language": "python",
   "name": "conda-env-generalmachinelearningforcpusonpython3_7vy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
